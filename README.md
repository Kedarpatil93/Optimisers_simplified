\# Optimizers Simplified :

This repository contains simplified explanations and implementations of various optimization algorithms used in machine learning. Currently, it includes an implementation of \*\*SGDRegressor\*\* using Scikit-Learn.

\## Contents

- \*\*SGDRegressor\_basic\_code.ipynb\*\*: A Jupyter Notebook demonstrating the use of Stochastic Gradient Descent (SGD) for linear regression.

\## 📖 Overview

Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used in machine learning to minimize loss functions. This notebook provides:

- A basic implementation of `SGDRegressor` with our own class (without use of sklearn)
- Explanation of hyperparameters such as learning rate and penalty.


\##  Requirements

To run this notebook, you need:

- Python 3.x
- Jupiter Notebook
- Required libraries: `numpy`, `pandas`, `matplotlib`, `sklearn`

You can install dependencies using:

\```sh

pip install numpy pandas matplotlib scikit-learn
